{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the steps to train a MLP Model. The notebook are separated into 3 main sections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- <a href='#MLP_1'>1. Import libraries and data</a>\n",
    "- <a href='#MLP_2'>2. Train MLP using pixel intensity as an input</a>\n",
    " - <a href='#MLP_2.1'>2.1 Find appropriate architecture by tuning hidden layer, hidden dimension and activation function</a> \n",
    " - <a href='#MLP_2.2'>2.2 Compared the performance of the networks </a>         \n",
    "- <a href='#MLP_3'>3. Train MLP using HOG descriptor as an input</a>\n",
    " - <a href='#MLP_3.1'>3.1 Find appropriate architecture by tuning hidden layer, hidden dimension and activation function</a> \n",
    " - <a href='#MLP_3.2'>3.2 Compared the performance of the networks </a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import libraries and data <a id='MLP_1'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "from skorch.callbacks import EarlyStopping\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skorch import NeuralNetClassifier\n",
    "import copy\n",
    "import time\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Function to show the first 5 images\n",
    "# Credit from INM427 Neural Computing Exercise\n",
    "def plot_example(X, y):\n",
    "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
    "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
    "        plt.subplot(151 + i)\n",
    "        plt.imshow(img,cmap ='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Get Current Path\n",
    "# Get data location path\n",
    "cwd = os.getcwd()\n",
    "script_path = cwd + '/'\n",
    "data_path = script_path + 'Data2'\n",
    "train_path = data_path +'/' + 'mnist_background_random_train.amat'\n",
    "test_path = data_path +'/' + 'mnist_background_random_test.amat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df_train = np.loadtxt(train_path)\n",
    "df_test = np.loadtxt(test_path)\n",
    "\n",
    "X_train = df_train[:,0:-1]\n",
    "y_train = df_train[:,-1]\n",
    "X_test = df_test[:,0:-1]\n",
    "y_test = df_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Change Datatype to tensor\n",
    "\n",
    "X_train  = torch.from_numpy(X_train).float()\n",
    "# converting the target into torch format\n",
    "y_train = torch.from_numpy(np.array(y_train))\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "\n",
    "#Formatting on testing set\n",
    "X_test  = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(np.array(y_test))\n",
    "y_test = y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper dictionary to convert the value to display\n",
    "func_dict = {1:'Relu',2:'LeakyReLu',3:'tanh',4:'sigmoid'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Train MLP using pixel intensity as an input  <a id='MLP_2'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method to train MLP is divided into 2 steps\n",
    "\n",
    "- 2.1 Find appropriate architecture by tuning hidden layer, hidden dimension and activation function\n",
    "  - 2.1.1 Create a network with 1 hidden layer\n",
    "  - 2.1.2 Tune the learning rate to the best 1 hidden layer architecture \n",
    "  - 2.1.3 Create a network with 2 hidden layers\n",
    "  - 2.1.4 Tune the learning rate to the best 2 hidden layer architecture\n",
    "\n",
    "- 2.2  Compared the performance of the networks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find appropriate architecture by tuning hidden layer, hidden dimension and activation function <a id='MLP_2.1'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create a network with 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general aritecture of the MLP will have the input as 784 (28*28 pixel),the output of 10.\n",
    "Softmax activation function at the output nodes and use cross entropy as a loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Set Class\n",
    "# MLP_1: 1 hidden layer Network\n",
    "\n",
    "class MLP_1(nn.Module):\n",
    "    def __init__(self,hidden_dim,function):\n",
    "        super(MLP_1,self).__init__()\n",
    "        self.fc1 = nn.Linear(784,hidden_dim)\n",
    "\n",
    "        if function ==1:\n",
    "            self.func = nn.ReLU()\n",
    "        elif function == 2:\n",
    "            self.func = nn.LeakyReLU()\n",
    "        elif function == 3:\n",
    "            self.func = nn.Tanh()\n",
    "        elif function ==4:\n",
    "            self.func = nn.Sigmoid()\n",
    "\n",
    "        self.output = nn.Linear(hidden_dim,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hidden = self.fc1(x)\n",
    "        hidden = self.func(hidden)\n",
    "\n",
    "        out = F.softmax(self.output(hidden), dim = -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Default parameter\n",
    "net = NeuralNetClassifier(module = MLP_1,\n",
    "                          module__hidden_dim = 50,\n",
    "                          module__function = 1,\n",
    "                          max_epochs = 500,\n",
    "                          criterion = nn.CrossEntropyLoss,\n",
    "                          lr = 0.1,\n",
    "                          batch_size = 100,\n",
    "                          callbacks= [EarlyStopping()],\n",
    "                          device= device,\n",
    "                          verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 81.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_1'>,\n",
       "  module__function=1,\n",
       "  module__hidden_dim=50,\n",
       "),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'module__function': [1, 2, 3, 4],\n",
       "                         'module__hidden_dim': [50, 100, 150, 200, 250, 300,\n",
       "                                                350, 400, 450, 500]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Gridseach to search for the best architecture\n",
    "hidden_list = np.arange(50, 550, 50).tolist()\n",
    "func_list = [1,2,3,4]\n",
    "\n",
    "param_grid = {'module__hidden_dim':hidden_list,\n",
    "              'module__function':func_list}\n",
    "grid_1 = GridSearchCV(net, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_module__function param_module__hidden_dim  mean_test_score  std_test_score\n",
      "0                    Relu                       50         0.727583        0.010895\n",
      "1                    Relu                      100         0.721250        0.037902\n",
      "2                    Relu                      150         0.723917        0.034604\n",
      "3                    Relu                      200         0.707417        0.030508\n",
      "4                    Relu                      250         0.727417        0.023099\n",
      "5                    Relu                      300         0.735000        0.011879\n",
      "6                    Relu                      350         0.719667        0.019478\n",
      "7                    Relu                      400         0.739833        0.036971\n",
      "8                    Relu                      450         0.718083        0.023014\n",
      "9                    Relu                      500         0.716583        0.032232\n",
      "10              LeakyReLu                       50         0.724500        0.037002\n",
      "11              LeakyReLu                      100         0.716083        0.034788\n",
      "12              LeakyReLu                      150         0.719083        0.040300\n",
      "13              LeakyReLu                      200         0.755583        0.024821\n",
      "14              LeakyReLu                      250         0.723583        0.040939\n",
      "15              LeakyReLu                      300         0.737333        0.007952\n",
      "16              LeakyReLu                      350         0.743750        0.028950\n",
      "17              LeakyReLu                      400         0.720250        0.039427\n",
      "18              LeakyReLu                      450         0.711333        0.035867\n",
      "19              LeakyReLu                      500         0.734250        0.026582\n",
      "20                   tanh                       50         0.790833        0.008878\n",
      "21                   tanh                      100         0.785667        0.014925\n",
      "22                   tanh                      150         0.783083        0.017084\n",
      "23                   tanh                      200         0.785917        0.016615\n",
      "24                   tanh                      250         0.784250        0.018317\n",
      "25                   tanh                      300         0.788500        0.011728\n",
      "26                   tanh                      350         0.788583        0.012158\n",
      "27                   tanh                      400         0.788250        0.014440\n",
      "28                   tanh                      450         0.790500        0.007779\n",
      "29                   tanh                      500         0.785667        0.011207\n",
      "30                sigmoid                       50         0.691583        0.048129\n",
      "31                sigmoid                      100         0.687750        0.007225\n",
      "32                sigmoid                      150         0.686833        0.005878\n",
      "33                sigmoid                      200         0.687583        0.007166\n",
      "34                sigmoid                      250         0.687167        0.004912\n",
      "35                sigmoid                      300         0.686583        0.008356\n",
      "36                sigmoid                      350         0.687500        0.006330\n",
      "37                sigmoid                      400         0.688250        0.004954\n",
      "38                sigmoid                      450         0.687500        0.006556\n",
      "39                sigmoid                      500         0.687083        0.007066\n"
     ]
    }
   ],
   "source": [
    "# %% Display the results\n",
    "df_temp1 = pd.DataFrame(grid_1.cv_results_)\n",
    "col = ['param_module__function','param_module__hidden_dim',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp1 =  df_temp1[col]\n",
    "df_temp1[\"param_module__function\"].replace(func_dict, inplace=True)\n",
    "print(df_temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_module__function param_module__hidden_dim  mean_test_score  std_test_score\n",
      "20                   tanh                       50         0.790833        0.008878\n",
      "28                   tanh                      450         0.790500        0.007779\n",
      "26                   tanh                      350         0.788583        0.012158\n",
      "25                   tanh                      300         0.788500        0.011728\n",
      "27                   tanh                      400         0.788250        0.014440\n",
      "23                   tanh                      200         0.785917        0.016615\n",
      "29                   tanh                      500         0.785667        0.011207\n",
      "21                   tanh                      100         0.785667        0.014925\n",
      "24                   tanh                      250         0.784250        0.018317\n",
      "22                   tanh                      150         0.783083        0.017084\n",
      "13              LeakyReLu                      200         0.755583        0.024821\n",
      "16              LeakyReLu                      350         0.743750        0.028950\n",
      "7                    Relu                      400         0.739833        0.036971\n",
      "15              LeakyReLu                      300         0.737333        0.007952\n",
      "5                    Relu                      300         0.735000        0.011879\n",
      "19              LeakyReLu                      500         0.734250        0.026582\n",
      "0                    Relu                       50         0.727583        0.010895\n",
      "4                    Relu                      250         0.727417        0.023099\n",
      "10              LeakyReLu                       50         0.724500        0.037002\n",
      "2                    Relu                      150         0.723917        0.034604\n"
     ]
    }
   ],
   "source": [
    "# Sort the result by accuracy score\n",
    "print(df_temp1.sort_values('mean_test_score', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture that provides the best mean test score is a small net work with just 50 dimension with a tanh activation funciton.\n",
    "The next step is to find the appropriate learning rate for this network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Tune the learning rate to the best 1 hidden layer architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed: 23.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_1'>,\n",
       "  module__function=1,\n",
       "  module__hidden_dim=50,\n",
       "),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lr': [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
       "                         'module__function': [3], 'module__hidden_dim': [50]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Gridseach to identify the best learning rate for the \n",
    "# previous network architecture (50 hidden nodes with activation function of tanh)\n",
    "hidden_list = [50]\n",
    "func_list = [3]\n",
    "lr_list = [1,0.5,0.1,0.05,0.01,0.005,0.001]\n",
    "\n",
    "param_grid = {'module__hidden_dim':hidden_list,\n",
    "              'module__function':func_list,\n",
    "             'lr':lr_list}\n",
    "\n",
    "\n",
    "grid_1_2 = GridSearchCV(net, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "grid_1_2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_lr  mean_test_score  std_test_score\n",
      "0        1         0.723917        0.040675\n",
      "1      0.5         0.731833        0.103863\n",
      "2      0.1         0.790167        0.008707\n",
      "3     0.05         0.794583        0.004986\n",
      "4     0.01         0.793083        0.024391\n",
      "5    0.005         0.754667        0.018605\n",
      "6    0.001         0.300333        0.031259\n"
     ]
    }
   ],
   "source": [
    "#Display the result\n",
    "df_temp1_2 = pd.DataFrame(grid_1_2.cv_results_)\n",
    "col = ['param_lr',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp1_2 =  df_temp1_2[col]\n",
    "print(df_temp1_2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaning rate of 0.05 perfrome the best, this will be use in a candidate model from a network of 1 hidden layer.<br>\n",
    "<b>Hence the final architecture for a MLP of 1 hidden layer is<b>\n",
    "* Hidden layer: 50\n",
    "* Activation Function: tanh\n",
    "* Learning Rate: 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Create a network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Set Class\n",
    "# MLP_2: 2 hidden layer\n",
    "\n",
    "class MLP_2(nn.Module):\n",
    "    def __init__(self,hidden_dim,function):\n",
    "        super(MLP_2,self).__init__()\n",
    "        \n",
    "        hid1, hid2 = hidden_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(784,hid1)\n",
    "        self.fc2 = nn.Linear(hid1,hid2)\n",
    "\n",
    "        if function ==1:\n",
    "            self.func = nn.ReLU()\n",
    "        elif function == 2:\n",
    "            self.func = nn.LeakyReLU()\n",
    "        elif function == 3:\n",
    "            self.func = nn.Tanh()\n",
    "        elif function ==4:\n",
    "            self.func = nn.Sigmoid()\n",
    "\n",
    "        self.output = nn.Linear(hid2,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hidden1 = self.fc1(x)\n",
    "        hidden1 = self.func(hidden1)\n",
    "\n",
    "        hidden2 = self.fc2(hidden1)\n",
    "        hidden2 = self.func(hidden2)\n",
    "        \n",
    "        out = F.softmax(self.output(hidden2), dim = -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Default parameter\n",
    "net_2 = NeuralNetClassifier(module = MLP_2,\n",
    "                          module__hidden_dim = (50,50),\n",
    "                          module__function = 1,\n",
    "                          max_epochs = 500,\n",
    "                          criterion = nn.CrossEntropyLoss,\n",
    "                          lr = 0.1,\n",
    "                          batch_size = 100,\n",
    "                          callbacks= [EarlyStopping()],\n",
    "                          device= device,\n",
    "                          verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  number of the second hidden layer is set to be half of the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 25),\n",
       " (100, 50),\n",
       " (150, 75),\n",
       " (200, 100),\n",
       " (250, 125),\n",
       " (300, 150),\n",
       " (350, 175),\n",
       " (400, 200),\n",
       " (450, 225),\n",
       " (500, 250)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up of combination of nodes of 2 hidden layer architect network\n",
    "hidden_1_list = np.arange(50, 550, 50).tolist()\n",
    "hidden_2_list = (np.array(hidden_1_list)/2).astype(int).tolist()\n",
    "\n",
    "\n",
    "hidden_2MLP_list = []\n",
    "for i in hidden_1_list:\n",
    "    for j in hidden_2_list:\n",
    "        if i/j ==2:\n",
    "            hidden_tuple = (i,j)\n",
    "            hidden_2MLP_list.append(hidden_tuple)\n",
    "hidden_2MLP_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 26.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_2'>,\n",
       "  module__function=1,\n",
       "  module__hidden_dim=(50, 50),\n",
       "),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'module__function': [1, 2, 3, 4],\n",
       "                         'module__hidden_dim': [(50, 25), (100, 50), (150, 75),\n",
       "                                                (200, 100), (250, 125),\n",
       "                                                (300, 150), (350, 175),\n",
       "                                                (400, 200), (450, 225),\n",
       "                                                (500, 250)]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Gridseach to search for the best architecture for 2 hidden layer network\n",
    "\n",
    "param_grid = {'module__hidden_dim':hidden_2MLP_list,\n",
    "              'module__function':func_list}\n",
    "\n",
    "grid_2 = GridSearchCV(net_2, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "grid_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_module__function param_module__hidden_dim  mean_test_score  std_test_score\n",
      "0                    Relu                 (50, 25)         0.624000        0.048539\n",
      "1                    Relu                (100, 50)         0.688167        0.052436\n",
      "2                    Relu                (150, 75)         0.655500        0.070039\n",
      "3                    Relu               (200, 100)         0.715250        0.041364\n",
      "4                    Relu               (250, 125)         0.661750        0.062902\n",
      "5                    Relu               (300, 150)         0.668500        0.044279\n",
      "6                    Relu               (350, 175)         0.656083        0.046216\n",
      "7                    Relu               (400, 200)         0.621583        0.028057\n",
      "8                    Relu               (450, 225)         0.647667        0.040942\n",
      "9                    Relu               (500, 250)         0.656583        0.057608\n",
      "10              LeakyReLu                 (50, 25)         0.641333        0.028533\n",
      "11              LeakyReLu                (100, 50)         0.683333        0.048465\n",
      "12              LeakyReLu                (150, 75)         0.645333        0.116966\n",
      "13              LeakyReLu               (200, 100)         0.701500        0.041800\n",
      "14              LeakyReLu               (250, 125)         0.655250        0.057170\n",
      "15              LeakyReLu               (300, 150)         0.676500        0.042657\n",
      "16              LeakyReLu               (350, 175)         0.666333        0.047672\n",
      "17              LeakyReLu               (400, 200)         0.718167        0.025576\n",
      "18              LeakyReLu               (450, 225)         0.711083        0.038358\n",
      "19              LeakyReLu               (500, 250)         0.702000        0.034514\n",
      "20                   tanh                 (50, 25)         0.707833        0.059560\n",
      "21                   tanh                (100, 50)         0.756417        0.024465\n",
      "22                   tanh                (150, 75)         0.747167        0.026965\n",
      "23                   tanh               (200, 100)         0.742417        0.033416\n",
      "24                   tanh               (250, 125)         0.758250        0.015121\n",
      "25                   tanh               (300, 150)         0.772250        0.024246\n",
      "26                   tanh               (350, 175)         0.739917        0.045696\n",
      "27                   tanh               (400, 200)         0.745667        0.039122\n",
      "28                   tanh               (450, 225)         0.751750        0.032881\n",
      "29                   tanh               (500, 250)         0.762500        0.006651\n",
      "30                sigmoid                 (50, 25)         0.110833        0.000000\n",
      "31                sigmoid                (100, 50)         0.110833        0.000000\n",
      "32                sigmoid                (150, 75)         0.110833        0.000000\n",
      "33                sigmoid               (200, 100)         0.110833        0.000000\n",
      "34                sigmoid               (250, 125)         0.110833        0.000000\n",
      "35                sigmoid               (300, 150)         0.110833        0.000000\n",
      "36                sigmoid               (350, 175)         0.110833        0.000000\n",
      "37                sigmoid               (400, 200)         0.110833        0.000000\n",
      "38                sigmoid               (450, 225)         0.110833        0.000000\n",
      "39                sigmoid               (500, 250)         0.112667        0.003667\n"
     ]
    }
   ],
   "source": [
    "# %%Display the result\n",
    "df_temp2 = pd.DataFrame(grid_2.cv_results_)\n",
    "col = ['param_module__function','param_module__hidden_dim',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp2 =  df_temp2[col]\n",
    "df_temp2[\"param_module__function\"].replace(func_dict, inplace=True)\n",
    "print(df_temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_module__function param_module__hidden_dim  mean_test_score  std_test_score\n",
      "25                   tanh               (300, 150)         0.772250        0.024246\n",
      "29                   tanh               (500, 250)         0.762500        0.006651\n",
      "24                   tanh               (250, 125)         0.758250        0.015121\n",
      "21                   tanh                (100, 50)         0.756417        0.024465\n",
      "28                   tanh               (450, 225)         0.751750        0.032881\n",
      "22                   tanh                (150, 75)         0.747167        0.026965\n",
      "27                   tanh               (400, 200)         0.745667        0.039122\n",
      "23                   tanh               (200, 100)         0.742417        0.033416\n",
      "26                   tanh               (350, 175)         0.739917        0.045696\n",
      "17              LeakyReLu               (400, 200)         0.718167        0.025576\n"
     ]
    }
   ],
   "source": [
    "# Sort the result by accuracy score\n",
    "print(df_temp2.sort_values('mean_test_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture that provides the best mean test score is a network with hidden layer of (300,150) dimension with a tanh activation funciton.\n",
    "The next step is to find the appropriate learning rate for this network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Tune the learning rate to the best 2 hidden layer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed: 22.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_2'>,\n",
       "  module__function=1,\n",
       "  module__hidden_dim=(50, 50),\n",
       "),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lr': [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
       "                         'module__function': [3],\n",
       "                         'module__hidden_dim': [(300, 150)]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Gridseach to identify the best learning rate for the \n",
    "# previous network architecture ((300,150) hidden nodes with activation function of tanh)\n",
    "lr_list =[1,0.5,0.1,0.05,0.01,0.005,0.001]\n",
    "\n",
    "param_grid = {'module__hidden_dim': [(300,150)],\n",
    "              'module__function':[3],\n",
    "              'lr': lr_list}\n",
    "\n",
    "grid_2_1 = GridSearchCV(net_2, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "grid_2_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_lr  mean_test_score  std_test_score\n",
      "0        1         0.744833        0.058387\n",
      "1      0.5         0.767583        0.055823\n",
      "2      0.1         0.764250        0.020312\n",
      "3     0.05         0.784333        0.011157\n",
      "4     0.01         0.783500        0.008782\n",
      "5    0.005         0.776417        0.025626\n",
      "6    0.001         0.098333        0.010541\n"
     ]
    }
   ],
   "source": [
    "# %%Display the result\n",
    "df_temp2_1 = pd.DataFrame(grid_2_1.cv_results_)\n",
    "col = ['param_lr',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp2_1 =  df_temp2_1[col]\n",
    "print(df_temp2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compared the performance of the networks <a id='MLP_2.2'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the tuning result of MLP using the pixel intensity as an input the best score for each MLP are\n",
    "\n",
    "<b>1. MLP with 1 hidden layer <b>\n",
    "* Mean_Score =0.795\n",
    "* Size of hidden layer = 50\n",
    "* Activation function = tanh\n",
    "* Learning Rate = 0.05\n",
    "\n",
    "<b>2. MLP with 2 hidden layer<b>\n",
    "* Mean_Score =0.784\n",
    "* Size of hidden layer (300,150)\n",
    "* Activation function = tanh\n",
    "* Learning Rate = 0.05\n",
    "\n",
    "The best aritecture is the one with only 1 hidden layer. This architecture will be trained using the whole dataset again and export to test with the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.3011\u001b[0m       \u001b[32m0.1800\u001b[0m        \u001b[35m2.2987\u001b[0m  0.4615\n",
      "      2        \u001b[36m2.2968\u001b[0m       \u001b[32m0.2258\u001b[0m        \u001b[35m2.2943\u001b[0m  0.2633\n",
      "      3        \u001b[36m2.2917\u001b[0m       0.2233        \u001b[35m2.2883\u001b[0m  0.2144\n",
      "      4        \u001b[36m2.2843\u001b[0m       0.2025        \u001b[35m2.2794\u001b[0m  0.2107\n",
      "      5        \u001b[36m2.2726\u001b[0m       0.2037        \u001b[35m2.2649\u001b[0m  0.2213\n",
      "      6        \u001b[36m2.2540\u001b[0m       0.2050        \u001b[35m2.2434\u001b[0m  0.2284\n",
      "      7        \u001b[36m2.2297\u001b[0m       0.2175        \u001b[35m2.2185\u001b[0m  0.2133\n",
      "      8        \u001b[36m2.2042\u001b[0m       \u001b[32m0.2992\u001b[0m        \u001b[35m2.1939\u001b[0m  0.2275\n",
      "      9        \u001b[36m2.1791\u001b[0m       \u001b[32m0.3725\u001b[0m        \u001b[35m2.1698\u001b[0m  0.2927\n",
      "     10        \u001b[36m2.1538\u001b[0m       \u001b[32m0.4129\u001b[0m        \u001b[35m2.1450\u001b[0m  0.2401\n",
      "     11        \u001b[36m2.1269\u001b[0m       \u001b[32m0.4508\u001b[0m        \u001b[35m2.1184\u001b[0m  0.2096\n",
      "     12        \u001b[36m2.0984\u001b[0m       \u001b[32m0.4779\u001b[0m        \u001b[35m2.0910\u001b[0m  0.2214\n",
      "     13        \u001b[36m2.0694\u001b[0m       \u001b[32m0.4975\u001b[0m        \u001b[35m2.0638\u001b[0m  0.2314\n",
      "     14        \u001b[36m2.0413\u001b[0m       \u001b[32m0.5158\u001b[0m        \u001b[35m2.0384\u001b[0m  0.2194\n",
      "     15        \u001b[36m2.0154\u001b[0m       \u001b[32m0.5225\u001b[0m        \u001b[35m2.0153\u001b[0m  0.2210\n",
      "     16        \u001b[36m1.9919\u001b[0m       \u001b[32m0.5396\u001b[0m        \u001b[35m1.9942\u001b[0m  0.2249\n",
      "     17        \u001b[36m1.9699\u001b[0m       \u001b[32m0.5758\u001b[0m        \u001b[35m1.9738\u001b[0m  0.2064\n",
      "     18        \u001b[36m1.9484\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.9539\u001b[0m  0.2409\n",
      "     19        \u001b[36m1.9282\u001b[0m       \u001b[32m0.6096\u001b[0m        \u001b[35m1.9362\u001b[0m  0.2986\n",
      "     20        \u001b[36m1.9109\u001b[0m       \u001b[32m0.6129\u001b[0m        \u001b[35m1.9215\u001b[0m  0.2206\n",
      "     21        \u001b[36m1.8967\u001b[0m       \u001b[32m0.6133\u001b[0m        \u001b[35m1.9096\u001b[0m  0.2925\n",
      "     22        \u001b[36m1.8851\u001b[0m       \u001b[32m0.6150\u001b[0m        \u001b[35m1.9001\u001b[0m  0.2572\n",
      "     23        \u001b[36m1.8755\u001b[0m       \u001b[32m0.6154\u001b[0m        \u001b[35m1.8923\u001b[0m  0.2218\n",
      "     24        \u001b[36m1.8674\u001b[0m       0.6146        \u001b[35m1.8857\u001b[0m  0.2234\n",
      "     25        \u001b[36m1.8603\u001b[0m       0.6150        \u001b[35m1.8799\u001b[0m  0.2395\n",
      "     26        \u001b[36m1.8538\u001b[0m       \u001b[32m0.6175\u001b[0m        \u001b[35m1.8746\u001b[0m  0.2200\n",
      "     27        \u001b[36m1.8475\u001b[0m       \u001b[32m0.6221\u001b[0m        \u001b[35m1.8693\u001b[0m  0.2135\n",
      "     28        \u001b[36m1.8410\u001b[0m       \u001b[32m0.6425\u001b[0m        \u001b[35m1.8635\u001b[0m  0.2634\n",
      "     29        \u001b[36m1.8336\u001b[0m       \u001b[32m0.6592\u001b[0m        \u001b[35m1.8567\u001b[0m  0.2329\n",
      "     30        \u001b[36m1.8249\u001b[0m       \u001b[32m0.6700\u001b[0m        \u001b[35m1.8488\u001b[0m  0.2583\n",
      "     31        \u001b[36m1.8154\u001b[0m       \u001b[32m0.6783\u001b[0m        \u001b[35m1.8406\u001b[0m  0.2191\n",
      "     32        \u001b[36m1.8061\u001b[0m       \u001b[32m0.6821\u001b[0m        \u001b[35m1.8327\u001b[0m  0.2289\n",
      "     33        \u001b[36m1.7971\u001b[0m       \u001b[32m0.6858\u001b[0m        \u001b[35m1.8253\u001b[0m  0.2741\n",
      "     34        \u001b[36m1.7886\u001b[0m       \u001b[32m0.6900\u001b[0m        \u001b[35m1.8182\u001b[0m  0.2198\n",
      "     35        \u001b[36m1.7803\u001b[0m       \u001b[32m0.6996\u001b[0m        \u001b[35m1.8112\u001b[0m  0.2967\n",
      "     36        \u001b[36m1.7717\u001b[0m       \u001b[32m0.7200\u001b[0m        \u001b[35m1.8038\u001b[0m  0.3172\n",
      "     37        \u001b[36m1.7628\u001b[0m       \u001b[32m0.7350\u001b[0m        \u001b[35m1.7961\u001b[0m  0.2310\n",
      "     38        \u001b[36m1.7537\u001b[0m       \u001b[32m0.7488\u001b[0m        \u001b[35m1.7883\u001b[0m  0.2335\n",
      "     39        \u001b[36m1.7447\u001b[0m       \u001b[32m0.7621\u001b[0m        \u001b[35m1.7806\u001b[0m  0.2270\n",
      "     40        \u001b[36m1.7360\u001b[0m       \u001b[32m0.7650\u001b[0m        \u001b[35m1.7734\u001b[0m  0.3325\n",
      "     41        \u001b[36m1.7277\u001b[0m       \u001b[32m0.7712\u001b[0m        \u001b[35m1.7666\u001b[0m  0.3720\n",
      "     42        \u001b[36m1.7200\u001b[0m       \u001b[32m0.7742\u001b[0m        \u001b[35m1.7603\u001b[0m  0.2196\n",
      "     43        \u001b[36m1.7128\u001b[0m       \u001b[32m0.7779\u001b[0m        \u001b[35m1.7546\u001b[0m  0.2457\n",
      "     44        \u001b[36m1.7060\u001b[0m       \u001b[32m0.7808\u001b[0m        \u001b[35m1.7494\u001b[0m  0.2320\n",
      "     45        \u001b[36m1.6997\u001b[0m       \u001b[32m0.7837\u001b[0m        \u001b[35m1.7445\u001b[0m  0.2341\n",
      "     46        \u001b[36m1.6939\u001b[0m       \u001b[32m0.7846\u001b[0m        \u001b[35m1.7401\u001b[0m  0.2303\n",
      "     47        \u001b[36m1.6884\u001b[0m       \u001b[32m0.7871\u001b[0m        \u001b[35m1.7360\u001b[0m  0.5773\n",
      "     48        \u001b[36m1.6832\u001b[0m       \u001b[32m0.7900\u001b[0m        \u001b[35m1.7323\u001b[0m  0.3202\n",
      "     49        \u001b[36m1.6784\u001b[0m       \u001b[32m0.7913\u001b[0m        \u001b[35m1.7288\u001b[0m  0.2410\n",
      "     50        \u001b[36m1.6738\u001b[0m       \u001b[32m0.7921\u001b[0m        \u001b[35m1.7256\u001b[0m  0.2385\n",
      "     51        \u001b[36m1.6695\u001b[0m       \u001b[32m0.7929\u001b[0m        \u001b[35m1.7227\u001b[0m  0.3422\n",
      "     52        \u001b[36m1.6654\u001b[0m       0.7921        \u001b[35m1.7199\u001b[0m  0.2179\n",
      "     53        \u001b[36m1.6615\u001b[0m       \u001b[32m0.7937\u001b[0m        \u001b[35m1.7174\u001b[0m  0.2274\n",
      "     54        \u001b[36m1.6578\u001b[0m       \u001b[32m0.7950\u001b[0m        \u001b[35m1.7151\u001b[0m  0.2549\n",
      "     55        \u001b[36m1.6543\u001b[0m       0.7950        \u001b[35m1.7129\u001b[0m  0.2369\n",
      "     56        \u001b[36m1.6509\u001b[0m       0.7950        \u001b[35m1.7109\u001b[0m  0.2869\n",
      "     57        \u001b[36m1.6477\u001b[0m       0.7946        \u001b[35m1.7091\u001b[0m  0.2391\n",
      "     58        \u001b[36m1.6446\u001b[0m       \u001b[32m0.7958\u001b[0m        \u001b[35m1.7074\u001b[0m  0.2559\n",
      "     59        \u001b[36m1.6417\u001b[0m       \u001b[32m0.7971\u001b[0m        \u001b[35m1.7058\u001b[0m  0.2296\n",
      "     60        \u001b[36m1.6389\u001b[0m       \u001b[32m0.7983\u001b[0m        \u001b[35m1.7043\u001b[0m  0.2786\n",
      "     61        \u001b[36m1.6361\u001b[0m       \u001b[32m0.7996\u001b[0m        \u001b[35m1.7029\u001b[0m  0.2438\n",
      "     62        \u001b[36m1.6335\u001b[0m       0.7996        \u001b[35m1.7016\u001b[0m  0.2232\n",
      "     63        \u001b[36m1.6310\u001b[0m       \u001b[32m0.8008\u001b[0m        \u001b[35m1.7004\u001b[0m  0.2198\n",
      "     64        \u001b[36m1.6286\u001b[0m       \u001b[32m0.8013\u001b[0m        \u001b[35m1.6992\u001b[0m  0.2900\n",
      "     65        \u001b[36m1.6262\u001b[0m       0.8013        \u001b[35m1.6982\u001b[0m  0.2442\n",
      "     66        \u001b[36m1.6239\u001b[0m       \u001b[32m0.8033\u001b[0m        \u001b[35m1.6972\u001b[0m  0.2347\n",
      "     67        \u001b[36m1.6217\u001b[0m       0.8025        \u001b[35m1.6963\u001b[0m  0.5308\n",
      "     68        \u001b[36m1.6196\u001b[0m       0.8021        \u001b[35m1.6954\u001b[0m  0.4257\n",
      "     69        \u001b[36m1.6175\u001b[0m       0.8017        \u001b[35m1.6946\u001b[0m  0.4746\n",
      "     70        \u001b[36m1.6155\u001b[0m       0.8029        \u001b[35m1.6938\u001b[0m  0.3095\n",
      "     71        \u001b[36m1.6136\u001b[0m       0.8017        \u001b[35m1.6931\u001b[0m  0.2591\n",
      "     72        \u001b[36m1.6117\u001b[0m       0.8021        \u001b[35m1.6924\u001b[0m  0.2384\n",
      "     73        \u001b[36m1.6099\u001b[0m       0.8025        \u001b[35m1.6917\u001b[0m  0.2286\n",
      "     74        \u001b[36m1.6081\u001b[0m       0.8025        \u001b[35m1.6911\u001b[0m  0.2590\n",
      "     75        \u001b[36m1.6063\u001b[0m       0.8033        \u001b[35m1.6906\u001b[0m  0.3233\n",
      "     76        \u001b[36m1.6046\u001b[0m       0.8029        \u001b[35m1.6900\u001b[0m  0.2860\n",
      "     77        \u001b[36m1.6030\u001b[0m       0.8029        \u001b[35m1.6895\u001b[0m  0.2915\n",
      "     78        \u001b[36m1.6014\u001b[0m       0.8025        \u001b[35m1.6890\u001b[0m  0.3131\n",
      "     79        \u001b[36m1.5998\u001b[0m       0.8025        \u001b[35m1.6885\u001b[0m  0.2321\n",
      "     80        \u001b[36m1.5983\u001b[0m       0.8013        \u001b[35m1.6881\u001b[0m  0.2672\n",
      "     81        \u001b[36m1.5968\u001b[0m       0.8008        \u001b[35m1.6877\u001b[0m  0.2726\n",
      "     82        \u001b[36m1.5954\u001b[0m       0.8000        \u001b[35m1.6873\u001b[0m  0.2382\n",
      "     83        \u001b[36m1.5940\u001b[0m       0.8013        \u001b[35m1.6869\u001b[0m  0.2222\n",
      "     84        \u001b[36m1.5926\u001b[0m       0.8008        \u001b[35m1.6865\u001b[0m  0.2257\n",
      "     85        \u001b[36m1.5912\u001b[0m       0.8017        \u001b[35m1.6862\u001b[0m  0.2559\n",
      "     86        \u001b[36m1.5899\u001b[0m       0.8021        \u001b[35m1.6859\u001b[0m  0.2297\n",
      "     87        \u001b[36m1.5887\u001b[0m       0.8013        \u001b[35m1.6856\u001b[0m  0.2332\n",
      "     88        \u001b[36m1.5874\u001b[0m       0.8013        \u001b[35m1.6853\u001b[0m  0.3466\n",
      "     89        \u001b[36m1.5862\u001b[0m       0.8021        \u001b[35m1.6850\u001b[0m  0.3851\n",
      "     90        \u001b[36m1.5850\u001b[0m       0.8021        \u001b[35m1.6847\u001b[0m  0.2206\n",
      "     91        \u001b[36m1.5838\u001b[0m       0.8021        \u001b[35m1.6845\u001b[0m  0.2384\n",
      "     92        \u001b[36m1.5827\u001b[0m       0.8025        \u001b[35m1.6843\u001b[0m  0.2285\n",
      "     93        \u001b[36m1.5815\u001b[0m       0.8021        \u001b[35m1.6840\u001b[0m  0.2743\n",
      "     94        \u001b[36m1.5804\u001b[0m       0.8021        \u001b[35m1.6838\u001b[0m  0.2584\n",
      "     95        \u001b[36m1.5794\u001b[0m       0.8025        \u001b[35m1.6836\u001b[0m  0.3198\n",
      "     96        \u001b[36m1.5783\u001b[0m       0.8021        \u001b[35m1.6834\u001b[0m  0.2498\n",
      "     97        \u001b[36m1.5773\u001b[0m       0.8017        \u001b[35m1.6833\u001b[0m  0.4001\n",
      "     98        \u001b[36m1.5762\u001b[0m       0.8021        \u001b[35m1.6831\u001b[0m  0.2946\n",
      "     99        \u001b[36m1.5752\u001b[0m       0.8025        \u001b[35m1.6829\u001b[0m  0.2593\n",
      "    100        \u001b[36m1.5743\u001b[0m       0.8013        \u001b[35m1.6828\u001b[0m  0.2252\n",
      "    101        \u001b[36m1.5733\u001b[0m       0.8013        \u001b[35m1.6826\u001b[0m  0.2173\n",
      "    102        \u001b[36m1.5724\u001b[0m       0.8008        \u001b[35m1.6825\u001b[0m  0.2254\n",
      "    103        \u001b[36m1.5714\u001b[0m       0.8008        \u001b[35m1.6824\u001b[0m  0.2229\n",
      "    104        \u001b[36m1.5705\u001b[0m       0.8013        \u001b[35m1.6823\u001b[0m  0.2165\n",
      "    105        \u001b[36m1.5696\u001b[0m       0.8013        \u001b[35m1.6821\u001b[0m  0.2110\n",
      "    106        \u001b[36m1.5687\u001b[0m       0.8004        \u001b[35m1.6820\u001b[0m  0.2176\n",
      "    107        \u001b[36m1.5679\u001b[0m       0.7992        \u001b[35m1.6820\u001b[0m  0.3378\n",
      "    108        \u001b[36m1.5670\u001b[0m       0.7987        \u001b[35m1.6819\u001b[0m  0.2797\n",
      "    109        \u001b[36m1.5662\u001b[0m       0.7983        \u001b[35m1.6818\u001b[0m  0.2531\n",
      "    110        \u001b[36m1.5654\u001b[0m       0.7983        \u001b[35m1.6817\u001b[0m  0.2136\n",
      "    111        \u001b[36m1.5646\u001b[0m       0.7983        \u001b[35m1.6817\u001b[0m  0.2144\n",
      "    112        \u001b[36m1.5638\u001b[0m       0.7979        \u001b[35m1.6816\u001b[0m  0.2211\n",
      "    113        \u001b[36m1.5630\u001b[0m       0.7963        \u001b[35m1.6816\u001b[0m  0.2180\n",
      "    114        \u001b[36m1.5623\u001b[0m       0.7950        \u001b[35m1.6815\u001b[0m  0.2488\n",
      "    115        \u001b[36m1.5615\u001b[0m       0.7946        \u001b[35m1.6815\u001b[0m  0.2213\n",
      "    116        \u001b[36m1.5608\u001b[0m       0.7942        \u001b[35m1.6814\u001b[0m  0.2152\n",
      "    117        \u001b[36m1.5601\u001b[0m       0.7942        \u001b[35m1.6814\u001b[0m  0.2416\n",
      "    118        \u001b[36m1.5594\u001b[0m       0.7937        \u001b[35m1.6813\u001b[0m  0.2452\n",
      "    119        \u001b[36m1.5587\u001b[0m       0.7942        \u001b[35m1.6813\u001b[0m  0.3519\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "#Set parameter\n",
    "net = NeuralNetClassifier(module = MLP_1,\n",
    "                          module__hidden_dim = 50,\n",
    "                          module__function = 3,\n",
    "                          max_epochs = 500,\n",
    "                          criterion = nn.CrossEntropyLoss,\n",
    "                          lr = 0.05,\n",
    "                          batch_size = 100,\n",
    "                          callbacks= [EarlyStopping()],\n",
    "                          device= device,\n",
    "                          verbose = 1)\n",
    "#Export Model\n",
    "t0 = time.time()\n",
    "net.fit(X_train,y_train)\n",
    "t1 = time.time()\n",
    "pkl_filename = \"MLP_pixel.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(net, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time 37.87144494056702\n"
     ]
    }
   ],
   "source": [
    "print('Fitting time',t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Train MLP using HOG descriptor as an input  <a id='MLP_3'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method to train MLP is divided into 2 steps\n",
    "\n",
    "- 3.1 Find appropriate architecture by tuning hidden layer, hidden dimension and activation function\n",
    "  - 3.1.1 Create a network with 1 hidden layer\n",
    "  - 3.1.2 Tune the learning rate to the best 1 hidden layer architecture \n",
    "  - 3.1.3 Create a network with 2 hidden layers\n",
    "  - 3.1.4 Tune the learning rate to the best 2 hidden layer architecture\n",
    "\n",
    "3.  Compared the performance of the networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to create a model and search for optimized parameters, we need to create some function and transform our data to HOG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Expects an array of 2d arrays (1 channel images)\n",
    "    Calculates hog features for each img\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y=None, orientations=8,\n",
    "                 pixels_per_cell=(2, 2),\n",
    "                 cells_per_block=(2, 2)):\n",
    "        self.y = y\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        def local_hog(X):\n",
    "            return hog(X,\n",
    "                       orientations=self.orientations,\n",
    "                       pixels_per_cell=self.pixels_per_cell,\n",
    "                       cells_per_block=self.cells_per_block)\n",
    "\n",
    "        try:  # parallel\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        except:\n",
    "            return np.array([local_hog(img) for img in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transform  numpy datatype to tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,y=None):\n",
    "        self.y =y\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        #def simpleconvert(x):\n",
    "        #    return \n",
    "        return torch.from_numpy(X).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redownload the data\n",
    "df_train = np.loadtxt(train_path)\n",
    "df_test = np.loadtxt(test_path)\n",
    "\n",
    "X_train = df_train[:,0:-1]\n",
    "y_train = df_train[:,-1]\n",
    "X_test = df_test[:,0:-1]\n",
    "y_test = df_test[:,-1]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28)\n",
    "\n",
    "y_train = torch.from_numpy(np.array(y_train))\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "\n",
    "y_test = torch.from_numpy(np.array(y_test))\n",
    "y_test = y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Find appropriate architecture by tuning hidden layer, hidden dimension and activation function <a id='MLP_3.1'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Create a network with 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general aritecture of the MLP in this section will have the input as 5408 (HOG feature),the output of 10.\n",
    "Softmax activation function at the output nodes and use cross entropy as a loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Set Class\n",
    "# MLP_3: 1 hidden layer; with Softmax at output layer\n",
    "\n",
    "class MLP_3_HOG(nn.Module):\n",
    "    def __init__(self,hidden_dim,function):\n",
    "        super(MLP_3_HOG,self).__init__()\n",
    "        self.fc1 = nn.Linear(5408,hidden_dim)\n",
    "\n",
    "        if function ==1:\n",
    "            self.func = nn.ReLU()\n",
    "        elif function == 2:\n",
    "            self.func = nn.LeakyReLU()\n",
    "        elif function == 3:\n",
    "            self.func = nn.Tanh()\n",
    "        elif function ==4:\n",
    "            self.func = nn.Sigmoid()\n",
    "\n",
    "        self.output = nn.Linear(hidden_dim,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #Change to tensor\n",
    "        hidden = self.fc1(x)\n",
    "        hidden = self.func(hidden)\n",
    "\n",
    "        out = F.softmax(self.output(hidden), dim = -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Default parameter\n",
    "net_HOG_1 = NeuralNetClassifier(module = MLP_3_HOG,\n",
    "                          module__hidden_dim = 50,\n",
    "                          module__function = 1,\n",
    "                          max_epochs = 500,\n",
    "                          criterion = nn.CrossEntropyLoss,\n",
    "                          lr = 0.1,\n",
    "                          batch_size = 100,\n",
    "                          callbacks= [EarlyStopping()],\n",
    "                          device= device,\n",
    "                          verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 44 candidates, totalling 220 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 26.4min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 215.1min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 491.4min\n",
      "[Parallel(n_jobs=-1)]: Done 220 out of 220 | elapsed: 525.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('hogify', HogTransformer()),\n",
       "                                       ('scalify', StandardScaler()),\n",
       "                                       ('dataloader', dataTransform()),\n",
       "                                       ('classify',\n",
       "                                        <class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_3_HOG'>,\n",
       "  module__function=1,\n",
       "  module__hidden_dim=50,\n",
       "))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classify__module__function': [1, 2, 3, 4],\n",
       "                         'classify__module__hidden_dim': [100, 300, 500, 700,\n",
       "                                                          900, 1100, 1300, 1500,\n",
       "                                                          1700, 1900, 2100]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Gridseach to search for the best architecture\n",
    "\n",
    "hidden_list = np.arange(100, 2200, 200).tolist()\n",
    "func_list = [1,2,3,4]\n",
    "\n",
    "# Initiate a pipleline to turn the image to HOG, standardize and train\n",
    "pipeline_1 = Pipeline(\n",
    "    [\n",
    "    ('hogify', HogTransformer(\n",
    "        pixels_per_cell=(2, 2),\n",
    "        cells_per_block=(2, 2),\n",
    "        orientations=8)\n",
    "     ),\n",
    "    ('scalify', StandardScaler()),\n",
    "        ('dataloader',dataTransform()),\n",
    "    ('classify',net_HOG_1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {'classify__module__hidden_dim':hidden_list,\n",
    "              'classify__module__function':func_list}\n",
    "grid_1_HOG = GridSearchCV(pipeline_1, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "\n",
    "grid_1_HOG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_classify__module__hidden_dim param_classify__module__function  mean_test_score  std_test_score\n",
      "0                                 100                                1         0.776500        0.008116\n",
      "1                                 300                                1         0.780750        0.005213\n",
      "2                                 500                                1         0.780583        0.005506\n",
      "3                                 700                                1         0.782250        0.006742\n",
      "4                                 900                                1         0.781167        0.006616\n",
      "5                                1100                                1         0.780000        0.006806\n",
      "6                                1300                                1         0.786500        0.003914\n",
      "7                                1500                                1         0.782000        0.004522\n",
      "8                                1700                                1         0.782000        0.008856\n",
      "9                                1900                                1         0.786583        0.008393\n",
      "10                               2100                                1         0.783333        0.008441\n",
      "11                                100                                2         0.771250        0.005368\n",
      "12                                300                                2         0.781000        0.006638\n",
      "13                                500                                2         0.778833        0.006425\n",
      "14                                700                                2         0.780500        0.007446\n",
      "15                                900                                2         0.782667        0.005606\n",
      "16                               1100                                2         0.782833        0.006310\n",
      "17                               1300                                2         0.779750        0.007494\n",
      "18                               1500                                2         0.782500        0.007250\n",
      "19                               1700                                2         0.784917        0.009027\n",
      "20                               1900                                2         0.780917        0.005691\n",
      "21                               2100                                2         0.781833        0.005424\n",
      "22                                100                                3         0.764083        0.009752\n",
      "23                                300                                3         0.765250        0.008861\n",
      "24                                500                                3         0.766667        0.004692\n",
      "25                                700                                3         0.767750        0.005175\n",
      "26                                900                                3         0.763417        0.005397\n",
      "27                               1100                                3         0.768583        0.007025\n",
      "28                               1300                                3         0.767250        0.004391\n",
      "29                               1500                                3         0.764833        0.005753\n",
      "30                               1700                                3         0.764750        0.006773\n",
      "31                               1900                                3         0.766500        0.006420\n",
      "32                               2100                                3         0.766667        0.006656\n",
      "33                                100                                4         0.750833        0.016806\n",
      "34                                300                                4         0.748583        0.020660\n",
      "35                                500                                4         0.742333        0.024575\n",
      "36                                700                                4         0.724083        0.020547\n",
      "37                                900                                4         0.706250        0.007298\n",
      "38                               1100                                4         0.728083        0.025920\n",
      "39                               1300                                4         0.707750        0.006127\n",
      "40                               1500                                4         0.692667        0.017253\n",
      "41                               1700                                4         0.703667        0.004883\n",
      "42                               1900                                4         0.714167        0.018452\n",
      "43                               2100                                4         0.713750        0.019460\n"
     ]
    }
   ],
   "source": [
    "#Display the result\n",
    "df_temp5 = pd.DataFrame(grid_1_HOG.cv_results_)\n",
    "col = ['param_classify__module__hidden_dim','param_classify__module__function',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp5 =  df_temp5[col]\n",
    "print(df_temp5)\n",
    "df_temp5.to_csv('MLP_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_classify__module__hidden_dim param_classify__module__function  mean_test_score  std_test_score\n",
      "9                                1900                                1         0.786583        0.008393\n",
      "6                                1300                                1         0.786500        0.003914\n",
      "19                               1700                                2         0.784917        0.009027\n",
      "10                               2100                                1         0.783333        0.008441\n",
      "16                               1100                                2         0.782833        0.006310\n",
      "15                                900                                2         0.782667        0.005606\n",
      "18                               1500                                2         0.782500        0.007250\n",
      "3                                 700                                1         0.782250        0.006742\n",
      "7                                1500                                1         0.782000        0.004522\n",
      "8                                1700                                1         0.782000        0.008856\n",
      "21                               2100                                2         0.781833        0.005424\n",
      "4                                 900                                1         0.781167        0.006616\n",
      "12                                300                                2         0.781000        0.006638\n",
      "20                               1900                                2         0.780917        0.005691\n",
      "1                                 300                                1         0.780750        0.005213\n",
      "2                                 500                                1         0.780583        0.005506\n",
      "14                                700                                2         0.780500        0.007446\n",
      "5                                1100                                1         0.780000        0.006806\n",
      "17                               1300                                2         0.779750        0.007494\n",
      "13                                500                                2         0.778833        0.006425\n"
     ]
    }
   ],
   "source": [
    "# Sort the result by accuracy score\n",
    "print(df_temp5.sort_values('mean_test_score', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean_test_score for the first top 20 is very close with the difference in the 3rd decimal place. The hidden nodes of 1900 ( with a Relu Activation) performs best with the mean test score of 0.787. However, the network of 1300 hidden nodes (Relu Activation)) also has a valdiation score that is in the same level. In this case we choose a smaller network (1300 hidden nodes) as a candidate for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the appropriate learning rate for the 1 hidden MLP with a hidden dimeansion of 1300 with a Relu activaiton function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Tune the learning rate to the best 1 hidden layer architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 39.4min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 146.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('hogify', HogTransformer()),\n",
       "                                       ('scalify', StandardScaler()),\n",
       "                                       ('dataloader', dataTransform()),\n",
       "                                       ('classify',\n",
       "                                        <class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_3_HOG'>,\n",
       "  module__function=1,\n",
       "  module__hidden_dim=50,\n",
       "))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classify__lr': [1, 0.5, 0.1, 0.05, 0.01, 0.005],\n",
       "                         'classify__module__function': [1],\n",
       "                         'classify__module__hidden_dim': [1300]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Gridseach to identify the best learning rate for the \n",
    "# previous network architecture (700 hidden nodes with activation function of leakyRelu)\n",
    "\n",
    "hidden_list =[1300]\n",
    "func_list = [1]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline_1 = Pipeline(\n",
    "    [\n",
    "    ('hogify', HogTransformer(\n",
    "        pixels_per_cell=(2, 2),\n",
    "        cells_per_block=(2, 2),\n",
    "        orientations=8)\n",
    "     ),\n",
    "    ('scalify', StandardScaler()),\n",
    "        ('dataloader',dataTransform()),\n",
    "    ('classify',net_HOG_1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# In this section lower limit of  learning rate is set to be 0.005 since from the preliminary test, \n",
    "# the learning rate pf 0.001 take too long\n",
    "lr_list = [1,0.5,0.1,0.05,0.01,0.005]\n",
    "\n",
    "param_grid = {'classify__module__hidden_dim':hidden_list,\n",
    "              'classify__module__function':func_list,\n",
    "             'classify__lr':lr_list}\n",
    "grid_2_HOG = GridSearchCV(pipeline_1, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "\n",
    "grid_2_HOG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_classify__lr  mean_test_score  std_test_score\n",
      "0                  1         0.748750        0.007737\n",
      "1                0.5         0.775417        0.006967\n",
      "2                0.1         0.785000        0.007509\n",
      "3               0.05         0.784500        0.007511\n",
      "4               0.01         0.784750        0.005340\n",
      "5              0.005         0.783583        0.006426\n"
     ]
    }
   ],
   "source": [
    "#Display the result\n",
    "df_temp6 = pd.DataFrame(grid_2_HOG.cv_results_)\n",
    "col = ['param_classify__lr',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp6 =  df_temp6[col]\n",
    "print(df_temp6)\n",
    "df_temp6.to_csv('MLP_HOG_1_LR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no major improvement when changing changing the learning rate, we will use th learning rate of 0.1 for this network.\n",
    "\n",
    "\n",
    "<b>Hence the final architecture for a MLP of 1 hidden layer is<b>\n",
    "* Hidden layer: 1300\n",
    "* Activation Function: Relu\n",
    "* Learning Rate: 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Create a network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Set Class\n",
    "# MLP_2: 2 hidden layer\n",
    "\n",
    "class MLP_4(nn.Module):\n",
    "    def __init__(self,hidden_dim,function):\n",
    "        super(MLP_4,self).__init__()\n",
    "        \n",
    "        hid1, hid2 = hidden_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(5408,hid1)\n",
    "        self.fc2 = nn.Linear(hid1,hid2)\n",
    "\n",
    "        if function ==1:\n",
    "            self.func = nn.ReLU()\n",
    "        elif function == 2:\n",
    "            self.func = nn.LeakyReLU()\n",
    "        elif function == 3:\n",
    "            self.func = nn.Tanh()\n",
    "        elif function ==4:\n",
    "            self.func = nn.Sigmoid()\n",
    "\n",
    "        self.output = nn.Linear(hid2,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hidden1 = self.fc1(x)\n",
    "        hidden1 = self.func(hidden1)\n",
    "\n",
    "        hidden2 = self.fc2(hidden1)\n",
    "        hidden2 = self.func(hidden2)\n",
    "        \n",
    "        out = F.softmax(self.output(hidden2), dim = -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 50),\n",
       " (300, 150),\n",
       " (500, 250),\n",
       " (700, 350),\n",
       " (900, 450),\n",
       " (1100, 550),\n",
       " (1300, 650),\n",
       " (1500, 750),\n",
       " (1700, 850),\n",
       " (1900, 950),\n",
       " (2100, 1050)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up of combination of nodes of 2 hidden layer architect network\n",
    "hidden_1_list = np.arange(100, 2200, 200).tolist()\n",
    "hidden_2_list = (np.array(hidden_1_list)/2).astype(int).tolist()\n",
    "func_list = [1,2,3,4]\n",
    "\n",
    "\n",
    "hidden_2MLP_list = []\n",
    "for i in hidden_1_list:\n",
    "    for j in hidden_2_list:\n",
    "        if i/j ==2:\n",
    "            hidden_tuple = (i,j)\n",
    "            hidden_2MLP_list.append(hidden_tuple)\n",
    "hidden_2MLP_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Default parameter\n",
    "net_HOG_3 = NeuralNetClassifier(module = MLP_4,\n",
    "                          module__hidden_dim = 50,\n",
    "                          module__function = 3,\n",
    "                          max_epochs = 500,\n",
    "                          criterion = nn.CrossEntropyLoss,\n",
    "                          lr = 0.1,\n",
    "                          batch_size = 100,\n",
    "                          callbacks= [EarlyStopping()],\n",
    "                          device= device,\n",
    "                          verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 44 candidates, totalling 220 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 35.2min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 297.7min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 836.9min\n",
      "[Parallel(n_jobs=-1)]: Done 220 out of 220 | elapsed: 920.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('hogify', HogTransformer()),\n",
       "                                       ('scalify', StandardScaler()),\n",
       "                                       ('dataloader', dataTransform()),\n",
       "                                       ('classify',\n",
       "                                        <class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_4'>,\n",
       "  module__function=3,\n",
       "  module__hidden_dim=50,\n",
       "))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classify__module__function': [1, 2, 3, 4],\n",
       "                         'classify__module__hidden_dim': [(100, 50), (300, 150),\n",
       "                                                          (500, 250),\n",
       "                                                          (700, 350),\n",
       "                                                          (900, 450),\n",
       "                                                          (1100, 550),\n",
       "                                                          (1300, 650),\n",
       "                                                          (1500, 750),\n",
       "                                                          (1700, 850),\n",
       "                                                          (1900, 950),\n",
       "                                                          (2100, 1050)]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Gridseach to search for the best architecture for 2 hidden layer network\n",
    "\n",
    "param_grid = {'classify__module__hidden_dim':hidden_2MLP_list,\n",
    "              'classify__module__function':func_list}\n",
    "\n",
    "pipeline_2 = Pipeline(\n",
    "    [\n",
    "    ('hogify', HogTransformer(\n",
    "        pixels_per_cell=(2, 2),\n",
    "        cells_per_block=(2, 2),\n",
    "        orientations=8)\n",
    "     ),\n",
    "    ('scalify', StandardScaler()),\n",
    "        ('dataloader',dataTransform()),\n",
    "    ('classify',net_HOG_3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "grid_3_HOG = GridSearchCV(pipeline_2, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "grid_3_HOG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_classify__module__hidden_dim param_classify__module__function  mean_test_score  std_test_score\n",
      "0                           (100, 50)                                1         0.770583        0.006533\n",
      "1                          (300, 150)                                1         0.778333        0.009923\n",
      "2                          (500, 250)                                1         0.779417        0.003967\n",
      "3                          (700, 350)                                1         0.782333        0.003797\n",
      "4                          (900, 450)                                1         0.782917        0.005633\n",
      "5                         (1100, 550)                                1         0.781083        0.002988\n",
      "6                         (1300, 650)                                1         0.781417        0.004997\n",
      "7                         (1500, 750)                                1         0.781833        0.006399\n",
      "8                         (1700, 850)                                1         0.782083        0.005815\n",
      "9                         (1900, 950)                                1         0.780750        0.006919\n",
      "10                       (2100, 1050)                                1         0.782083        0.005761\n",
      "11                          (100, 50)                                2         0.771667        0.005089\n",
      "12                         (300, 150)                                2         0.778167        0.005825\n",
      "13                         (500, 250)                                2         0.780250        0.007554\n",
      "14                         (700, 350)                                2         0.781167        0.004620\n",
      "15                         (900, 450)                                2         0.780583        0.003797\n",
      "16                        (1100, 550)                                2         0.777583        0.002587\n",
      "17                        (1300, 650)                                2         0.782333        0.007645\n",
      "18                        (1500, 750)                                2         0.781333        0.005586\n",
      "19                        (1700, 850)                                2         0.780250        0.005637\n",
      "20                        (1900, 950)                                2         0.779333        0.006512\n",
      "21                       (2100, 1050)                                2         0.779333        0.003967\n",
      "22                          (100, 50)                                3         0.749167        0.005755\n",
      "23                         (300, 150)                                3         0.760250        0.006393\n",
      "24                         (500, 250)                                3         0.761750        0.006637\n",
      "25                         (700, 350)                                3         0.764167        0.008886\n",
      "26                         (900, 450)                                3         0.766083        0.007396\n",
      "27                        (1100, 550)                                3         0.769083        0.006929\n",
      "28                        (1300, 650)                                3         0.766083        0.006155\n",
      "29                        (1500, 750)                                3         0.767417        0.008260\n",
      "30                        (1700, 850)                                3         0.767917        0.006319\n",
      "31                        (1900, 950)                                3         0.768250        0.006793\n",
      "32                       (2100, 1050)                                3         0.767083        0.005839\n",
      "33                          (100, 50)                                4         0.660750        0.028509\n",
      "34                         (300, 150)                                4         0.639417        0.019110\n",
      "35                         (500, 250)                                4         0.627750        0.032168\n",
      "36                         (700, 350)                                4         0.625083        0.019310\n",
      "37                         (900, 450)                                4         0.618417        0.030538\n",
      "38                        (1100, 550)                                4         0.621083        0.018190\n",
      "39                        (1300, 650)                                4         0.623833        0.015038\n",
      "40                        (1500, 750)                                4         0.624250        0.033500\n",
      "41                        (1700, 850)                                4         0.628333        0.019413\n",
      "42                        (1900, 950)                                4         0.627750        0.022014\n",
      "43                       (2100, 1050)                                4         0.533000        0.214590\n"
     ]
    }
   ],
   "source": [
    "#Display the result\n",
    "df_temp7 = pd.DataFrame(grid_3_HOG.cv_results_)\n",
    "col = ['param_classify__module__hidden_dim','param_classify__module__function',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp7 =  df_temp7[col]\n",
    "print(df_temp7)\n",
    "df_temp7.to_csv('MLP_HOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_classify__module__hidden_dim param_classify__module__function  mean_test_score  std_test_score\n",
      "4                          (900, 450)                                1         0.782917        0.005633\n",
      "17                        (1300, 650)                                2         0.782333        0.007645\n",
      "3                          (700, 350)                                1         0.782333        0.003797\n",
      "8                         (1700, 850)                                1         0.782083        0.005815\n",
      "10                       (2100, 1050)                                1         0.782083        0.005761\n",
      "7                         (1500, 750)                                1         0.781833        0.006399\n",
      "6                         (1300, 650)                                1         0.781417        0.004997\n",
      "18                        (1500, 750)                                2         0.781333        0.005586\n",
      "14                         (700, 350)                                2         0.781167        0.004620\n",
      "5                         (1100, 550)                                1         0.781083        0.002988\n",
      "9                         (1900, 950)                                1         0.780750        0.006919\n",
      "15                         (900, 450)                                2         0.780583        0.003797\n",
      "19                        (1700, 850)                                2         0.780250        0.005637\n",
      "13                         (500, 250)                                2         0.780250        0.007554\n",
      "2                          (500, 250)                                1         0.779417        0.003967\n"
     ]
    }
   ],
   "source": [
    "# Sort the result by accuracy score\n",
    "print(df_temp7.sort_values('mean_test_score', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean_test_score for the first top 15 is very close with the difference in the 3rd decimal place. The best network is a network of 900 and 450 hidden nodes and a Relu Activation Function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the good learning rate for the 2 hidden MLP with a hidden dimeansion of (900,450) with a Relu activaiton function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Tune the learning rate to the best 2 hidden layer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 129.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('hogify', HogTransformer()),\n",
       "                                       ('scalify', StandardScaler()),\n",
       "                                       ('dataloader', dataTransform()),\n",
       "                                       ('classify',\n",
       "                                        <class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.MLP_4'>,\n",
       "  module__function=3,\n",
       "  module__hidden_dim=50,\n",
       "))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classify__lr': [1, 0.5, 0.1, 0.05, 0.01, 0.005],\n",
       "                         'classify__module__function': [1],\n",
       "                         'classify__module__hidden_dim': [(900, 450)]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting gridsearch for 2 hidden layer network\n",
    "hidden_list =[(900,450)]\n",
    "func_list = [1]\n",
    "lr_list = [1,0.5,0.1,0.05,0.01,0.005]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'classify__module__hidden_dim':hidden_list,\n",
    "              'classify__module__function':func_list,\n",
    "              'classify__lr':lr_list}\n",
    "\n",
    "grid_4_HOG = GridSearchCV(pipeline_2, param_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv= 5,\n",
    "                    verbose = 4,\n",
    "                    return_train_score = True)\n",
    "\n",
    "grid_4_HOG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_classify__lr  mean_test_score  std_test_score\n",
      "0                  1         0.718833        0.008260\n",
      "1                0.5         0.758083        0.007784\n",
      "2                0.1         0.780667        0.004071\n",
      "3               0.05         0.783250        0.007295\n",
      "4               0.01         0.780250        0.003255\n",
      "5              0.005         0.782167        0.005207\n"
     ]
    }
   ],
   "source": [
    "#Display the result\n",
    "df_temp8 = pd.DataFrame(grid_4_HOG.cv_results_)\n",
    "col = ['param_classify__lr',\n",
    "       'mean_test_score','std_test_score']\n",
    "\n",
    "\n",
    "df_temp8 =  df_temp8[col]\n",
    "print(df_temp8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performance is learning rate of 0.05 with a mean_test_score of 0.783\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compared the performance of the networks <a id='MLP_3.2'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the tuning result of MLP using the HOG as an input the best score for each MLP are\n",
    "\n",
    "<b>1. MLP with 1 hidden layer <b>\n",
    "* Mean_Score =0.785\n",
    "* Size of hidden layer 1900\n",
    "* Activation function = Relu\n",
    "* Learning Rate = 0.1\n",
    "\n",
    "<b>2. MLP with 2 hidden layer<b>\n",
    "* Mean_Score =0.783\n",
    "* Size of hidden layer (900,450)\n",
    "* Activation function = Relu\n",
    "* Learning Rate = 0.05\n",
    "\n",
    "The best aritecture is the one with only 1 hidden layer. This architecture will be trained using the whole dataset again and export to test with the test set in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.2275\u001b[0m       \u001b[32m0.4458\u001b[0m        \u001b[35m2.0967\u001b[0m  2.2530\n",
      "      2        \u001b[36m1.9743\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m1.9105\u001b[0m  2.1615\n",
      "      3        \u001b[36m1.7800\u001b[0m       \u001b[32m0.8037\u001b[0m        \u001b[35m1.7620\u001b[0m  2.1785\n",
      "      4        \u001b[36m1.6398\u001b[0m       \u001b[32m0.8137\u001b[0m        \u001b[35m1.7133\u001b[0m  2.2684\n",
      "      5        \u001b[36m1.5747\u001b[0m       \u001b[32m0.8150\u001b[0m        \u001b[35m1.6983\u001b[0m  2.1817\n",
      "      6        \u001b[36m1.5400\u001b[0m       0.8096        \u001b[35m1.6919\u001b[0m  2.1577\n",
      "      7        \u001b[36m1.5204\u001b[0m       0.8063        \u001b[35m1.6884\u001b[0m  2.1714\n",
      "      8        \u001b[36m1.5086\u001b[0m       0.8046        \u001b[35m1.6862\u001b[0m  2.1698\n",
      "      9        \u001b[36m1.5007\u001b[0m       0.8054        \u001b[35m1.6844\u001b[0m  2.1809\n",
      "     10        \u001b[36m1.4954\u001b[0m       0.8042        \u001b[35m1.6828\u001b[0m  2.1814\n",
      "     11        \u001b[36m1.4915\u001b[0m       0.8029        \u001b[35m1.6815\u001b[0m  2.1751\n",
      "     12        \u001b[36m1.4885\u001b[0m       0.8037        \u001b[35m1.6805\u001b[0m  2.1645\n",
      "     13        \u001b[36m1.4860\u001b[0m       0.8037        \u001b[35m1.6796\u001b[0m  2.1663\n",
      "     14        \u001b[36m1.4842\u001b[0m       0.8054        \u001b[35m1.6788\u001b[0m  2.1708\n",
      "     15        \u001b[36m1.4830\u001b[0m       0.8050        \u001b[35m1.6783\u001b[0m  2.1605\n",
      "     16        \u001b[36m1.4817\u001b[0m       0.8046        \u001b[35m1.6778\u001b[0m  2.1745\n",
      "     17        \u001b[36m1.4807\u001b[0m       0.8025        \u001b[35m1.6773\u001b[0m  2.1728\n",
      "     18        \u001b[36m1.4799\u001b[0m       0.8029        \u001b[35m1.6769\u001b[0m  2.1520\n",
      "     19        \u001b[36m1.4792\u001b[0m       0.8021        \u001b[35m1.6766\u001b[0m  2.1604\n",
      "     20        \u001b[36m1.4785\u001b[0m       0.8017        \u001b[35m1.6763\u001b[0m  2.1567\n",
      "     21        \u001b[36m1.4780\u001b[0m       0.8017        \u001b[35m1.6760\u001b[0m  2.1635\n",
      "     22        \u001b[36m1.4775\u001b[0m       0.8025        \u001b[35m1.6756\u001b[0m  2.1683\n",
      "     23        \u001b[36m1.4770\u001b[0m       0.8029        \u001b[35m1.6753\u001b[0m  2.1572\n",
      "     24        \u001b[36m1.4767\u001b[0m       0.8029        \u001b[35m1.6749\u001b[0m  2.1785\n",
      "     25        \u001b[36m1.4764\u001b[0m       0.8025        \u001b[35m1.6746\u001b[0m  2.1777\n",
      "     26        \u001b[36m1.4761\u001b[0m       0.8029        \u001b[35m1.6743\u001b[0m  2.1647\n",
      "     27        \u001b[36m1.4758\u001b[0m       0.8029        \u001b[35m1.6741\u001b[0m  2.1528\n",
      "     28        \u001b[36m1.4754\u001b[0m       0.8037        \u001b[35m1.6738\u001b[0m  2.1490\n",
      "     29        \u001b[36m1.4751\u001b[0m       0.8033        \u001b[35m1.6735\u001b[0m  2.1533\n",
      "     30        \u001b[36m1.4749\u001b[0m       0.8050        \u001b[35m1.6732\u001b[0m  2.1524\n",
      "     31        \u001b[36m1.4746\u001b[0m       0.8042        \u001b[35m1.6729\u001b[0m  2.1702\n",
      "     32        \u001b[36m1.4743\u001b[0m       0.8050        \u001b[35m1.6727\u001b[0m  2.1585\n",
      "     33        \u001b[36m1.4740\u001b[0m       0.8054        \u001b[35m1.6725\u001b[0m  2.1491\n",
      "     34        \u001b[36m1.4739\u001b[0m       0.8054        \u001b[35m1.6724\u001b[0m  2.1811\n",
      "     35        \u001b[36m1.4738\u001b[0m       0.8054        \u001b[35m1.6723\u001b[0m  2.1542\n",
      "     36        \u001b[36m1.4737\u001b[0m       0.8054        \u001b[35m1.6721\u001b[0m  2.1529\n",
      "     37        \u001b[36m1.4736\u001b[0m       0.8046        \u001b[35m1.6720\u001b[0m  2.1482\n",
      "     38        \u001b[36m1.4736\u001b[0m       0.8046        \u001b[35m1.6719\u001b[0m  2.2355\n",
      "     39        \u001b[36m1.4735\u001b[0m       0.8054        \u001b[35m1.6718\u001b[0m  2.2057\n",
      "     40        \u001b[36m1.4734\u001b[0m       0.8050        \u001b[35m1.6717\u001b[0m  2.1526\n",
      "     41        \u001b[36m1.4732\u001b[0m       0.8054        \u001b[35m1.6715\u001b[0m  2.1393\n",
      "     42        \u001b[36m1.4731\u001b[0m       0.8050        \u001b[35m1.6714\u001b[0m  2.1514\n",
      "     43        \u001b[36m1.4730\u001b[0m       0.8050        \u001b[35m1.6713\u001b[0m  2.1388\n",
      "     44        \u001b[36m1.4729\u001b[0m       0.8046        \u001b[35m1.6712\u001b[0m  2.1470\n",
      "     45        \u001b[36m1.4728\u001b[0m       0.8037        1.6713  2.1493\n",
      "     46        \u001b[36m1.4726\u001b[0m       0.8033        1.6713  2.1440\n",
      "     47        \u001b[36m1.4725\u001b[0m       0.8037        \u001b[35m1.6712\u001b[0m  2.1413\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "#Set Default parameter\n",
    "net_HOG_1 = NeuralNetClassifier(module = MLP_3_HOG,\n",
    "                          module__hidden_dim = 700,\n",
    "                          module__function = 1,\n",
    "                          max_epochs = 500,\n",
    "                          criterion = nn.CrossEntropyLoss,\n",
    "                          lr = 0.1,\n",
    "                          batch_size = 100,\n",
    "                          callbacks= [EarlyStopping()],\n",
    "                          device= device,\n",
    "                          verbose = 1)\n",
    "\n",
    "\n",
    "pipeline_1 = Pipeline(\n",
    "    [\n",
    "    ('hogify', HogTransformer(\n",
    "        pixels_per_cell=(2, 2),\n",
    "        cells_per_block=(2, 2),\n",
    "        orientations=8)\n",
    "     ),\n",
    "    ('scalify', StandardScaler()),\n",
    "        ('dataloader',dataTransform()),\n",
    "    ('classify',net_HOG_1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline_1.fit(X_train,y_train)\n",
    "t1 =time.time()\n",
    "pkl_filename = \"MLP_hog.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(pipeline_1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time 151.33455419540405\n"
     ]
    }
   ],
   "source": [
    "print('Fitting time',t1-t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
