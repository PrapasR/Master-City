{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Test of Proximal Policy Optimization (PPO)\n",
    "### By Nattaphat Thanaussawanun and Prapas Rakchartkiattikul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- <a href='#ev_1'>Test of the experiment 1: the small environment (N = 20)</a>\n",
    "- <a href='#ev_2'>Test of the experiment 2: the medium environment (N = 40)</a>\n",
    "- <a href='#ev_3'>Test of the experiment 3: the large environment (N = 60)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo_algorithm import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help function to run several experiments\n",
    "def run_experiments(env, actor_model, num_exps):\n",
    "    with torch.no_grad():\n",
    "        all_rewards = []\n",
    "        all_action_counts = []\n",
    "\n",
    "        for _ in range(num_exps):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "            total_reward = 0\n",
    "            action_count = 0\n",
    "\n",
    "            while not done:\n",
    "                dist = Categorical(actor_model(obs))\n",
    "                action_index = torch.squeeze(dist.sample()).item()\n",
    "                action_name = index_to_actions[action_index].name\n",
    "                obs, reward, done = env.step(action_name)\n",
    "                total_reward += reward\n",
    "                action_count += 1\n",
    "\n",
    "            all_rewards.append(total_reward)\n",
    "            all_action_counts.append(action_count)\n",
    "\n",
    "        max_reward = max(all_rewards)\n",
    "        avg_reward = np.mean(all_rewards)\n",
    "        var_reward = np.std(all_rewards)\n",
    "        avg_action_count = np.mean(all_action_counts)\n",
    "\n",
    "    return max_reward, avg_reward, var_reward, avg_action_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of the experiment 1: the small environment (N = 20) <a id='ev_1'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X \n",
      "X . . . . . L . . . . . . . . . . . . X \n",
      "X . . . . . . . L . . . . L . . . . . X \n",
      "X . . . . . . . . . . . . . X . . . . X \n",
      "X . . . . . . . . . . . . . . L . . . X \n",
      "X . . . . . . . . E . . . . X . X . . X \n",
      "X . X . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . X . L . . . . . . . . . X \n",
      "X . . X . . . . . . . . . . . . X . . X \n",
      "X . . . . . . . X . . . . . . . . . . X \n",
      "X . . . . L . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . L . . . . . . . . X \n",
      "X . . . . . . . . . . . L . . . . . . X \n",
      "X . . . . L . . . . . . . . . . . . . X \n",
      "X . . . . . . . . L . . . . . . . . . X \n",
      "X . A . . . . . . . . . . . . . . . . X \n",
      "X . . . . X . . . . . . . . . . . . . X \n",
      "X X X X X X X X X X X X X X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Actor model\n",
    "PATH_MLP = 'Actor_Model_20.pth'\n",
    "model = torch.load(PATH_MLP)\n",
    "model.eval()\n",
    "\n",
    "# Import the same environment as training\n",
    "FILENAME = open('dungeon_20.p', 'rb')\n",
    "dungeon = pickle.load(FILENAME)\n",
    "FILENAME.close()\n",
    "\n",
    "dungeon.reset()\n",
    "dungeon.display()\n",
    "\n",
    "# Running the 1000 experiments\n",
    "max_r, avg_r, var_r, avg_ac = run_experiments(dungeon, model, 1000)\n",
    "avg_r_per_ac = avg_r/avg_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment size (20 x 20) of the dungeon shows the following result:\n",
      "Maximum Reward: 399.00 | Average Reward: 299.82 | Variance Reward: 114.88 | Average Action: 62.51 | Average Reward Per Action: 4.80\n"
     ]
    }
   ],
   "source": [
    "print(f'The environment size ({dungeon.size} x {dungeon.size}) of the dungeon shows the following result:')\n",
    "print(f'Maximum Reward: {max_r:0.2f} | Average Reward: {avg_r:0.2f} | Variance Reward: {var_r:0.2f} | '\n",
    "          f'Average Action: {avg_ac:0.2f} | Average Reward Per Action: {avg_r_per_ac:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of the experiment 2: the medium environment (N = 40) <a id='ev_2'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . X \n",
      "X X . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . L . X . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . L . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . E . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . X . . . . . X \n",
      "X . . . . . . . . . . L . . . X . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . X X \n",
      "X . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . X . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . X . . . . . . X . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . A . . . . . . . . . . . . . . . L . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . X \n",
      "X . . . . . . . . X . . . . . . . . . . . . . . . . . L . . . . . . . . . . . X \n",
      "X . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . X \n",
      "X . . . . . . . . . . L . . . . . . . . . X . . . . . . . L . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . X \n",
      "X L . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . X \n",
      "X . L . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Actor model\n",
    "PATH_MLP = 'Actor_Model_40.pth'\n",
    "model = torch.load(PATH_MLP)\n",
    "model.eval()\n",
    "\n",
    "# Import the same environment as training\n",
    "FILENAME = open('dungeon_40.p', 'rb')\n",
    "dungeon = pickle.load(FILENAME)\n",
    "FILENAME.close()\n",
    "\n",
    "dungeon.reset()\n",
    "dungeon.display()\n",
    "\n",
    "# Running the 1000 experiments\n",
    "max_r, avg_r, var_r, avg_ac = run_experiments(dungeon, model, 1000)\n",
    "avg_r_per_ac = avg_r/avg_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment size (40 x 40) of the dungeon shows the following result:\n",
      "Maximum Reward: 1599.00 | Average Reward: 1405.11 | Variance Reward: 147.25 | Average Action: 147.35 | Average Reward Per Action: 9.54\n"
     ]
    }
   ],
   "source": [
    "print(f'The environment size ({dungeon.size} x {dungeon.size}) of the dungeon shows the following result:')\n",
    "print(f'Maximum Reward: {max_r:0.2f} | Average Reward: {avg_r:0.2f} | Variance Reward: {var_r:0.2f} | '\n",
    "          f'Average Action: {avg_ac:0.2f} | Average Reward Per Action: {avg_r_per_ac:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of the experiment 3: the large environment (N = 60) <a id='ev_3'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X \n",
      "X . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . X . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . X . . . . . . . . . . . . X \n",
      "X . . L . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . X \n",
      "X . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . X . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . X \n",
      "X . . . . . . . . . . . X . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X L . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . L . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . X . . . . . . . . . . . . . L X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . . . . L . X . . . X \n",
      "X . . . . . . . . . . . . L . . . . . . . . . . . . . . L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . X \n",
      "X . . . L . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . L . . . . . . . . . . . . A . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . . . L X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . L . . . L . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X \n",
      "X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . X X \n",
      "X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Actor model\n",
    "PATH_MLP = 'Actor_Model_60.pth'\n",
    "model = torch.load(PATH_MLP)\n",
    "model.eval()\n",
    "\n",
    "# Import the same environment as training\n",
    "FILENAME = open('dungeon_60.p', 'rb')\n",
    "dungeon = pickle.load(FILENAME)\n",
    "FILENAME.close()\n",
    "\n",
    "dungeon.reset()\n",
    "dungeon.display()\n",
    "\n",
    "# Running the 1000 experiments\n",
    "max_r, avg_r, var_r, avg_ac = run_experiments(dungeon, model, 1000)\n",
    "avg_r_per_ac = avg_r/avg_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment size (60 x 60) of the dungeon shows the following result:\n",
      "Maximum Reward: 3599.00 | Average Reward: 3516.16 | Variance Reward: 52.22 | Average Action: 51.91 | Average Reward Per Action: 67.74\n"
     ]
    }
   ],
   "source": [
    "print(f'The environment size ({dungeon.size} x {dungeon.size}) of the dungeon shows the following result:')\n",
    "print(f'Maximum Reward: {max_r:0.2f} | Average Reward: {avg_r:0.2f} | Variance Reward: {var_r:0.2f} | '\n",
    "          f'Average Action: {avg_ac:0.2f} | Average Reward Per Action: {avg_r_per_ac:0.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
